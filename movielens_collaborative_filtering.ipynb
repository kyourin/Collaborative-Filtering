{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXy7AQO7qBXt"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from datetime import datetime\n",
        "from sortedcontainers import SortedList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehSlsNgwqcZ5"
      },
      "outputs": [],
      "source": [
        "with open('user2movie.json', 'rb') as f:\n",
        "  user2movie = pickle.load(f)\n",
        "with open('movie2user.json', 'rb') as f:\n",
        "  movie2user = pickle.load(f)\n",
        "with open('usermovie2rating.json', 'rb') as f:\n",
        "  usermovie2rating = pickle.load(f)\n",
        "with open('usermovie2rating_test.json', 'rb') as f:\n",
        "  usermovie2rating_test = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYrOypdMrJwb",
        "outputId": "b43f1d49-8888-4c71-92e1-3c6d38573f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N: 10000 M: 2000\n"
          ]
        }
      ],
      "source": [
        "N = np.max(list(user2movie.keys())) + 1\n",
        "m1 = np.max(list(movie2user.keys()))\n",
        "m2 = np.max([m for (u,m),r in usermovie2rating_test.items()])\n",
        "M = max(m1, m2) + 1\n",
        "print('N:', N, 'M:', M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybxZR5Oauvca"
      },
      "outputs": [],
      "source": [
        "# to find the user similarities, you have to do O(N^2 * M) calculations\n",
        "# in the real-world you'd want to parallelize this\n",
        "# note: we only have to do half calculations, since w_ij is symmetric\n",
        "K = 25 # number of neighbours we'd like to consider\n",
        "limit = 5 # number of common movies users must have in common in order to consider\n",
        "neighbours = [] # store neighbours in this list\n",
        "averages = [] # each user's average rating for later use\n",
        "deviations = [] # each user's deviation for later use\n",
        "for i in range(N):\n",
        "  # find the 25 closest users to user i\n",
        "  movies_i = user2movie[i]\n",
        "  movies_i_set = set(movies_i)\n",
        "\n",
        "  # calculate avg and deviation\n",
        "  ratings_i = {movie:usermovie2rating[(i,movie)] for movie in movies_i}\n",
        "  avg_i = np.mean(list(ratings_i.values()))\n",
        "  dev_i = {movie:(rating-avg_i) for movie, rating in ratings_i.items()}\n",
        "  dev_i_values = np.array(list(dev_i.values()))\n",
        "  sigma_i = np.sqrt(dev_i_values.dot(dev_i_values))\n",
        "  \n",
        "  # save these for later use\n",
        "  averages.append(avg_i)\n",
        "  deviations.append(dev_i)\n",
        "\n",
        "  sl = SortedList()\n",
        "  for j in range(N):\n",
        "    if j != i:\n",
        "      movies_j = user2movie[j]\n",
        "      movies_j_set = set(movies_j)\n",
        "      common_movies = (movies_i_set & movies_j_set) # intersection\n",
        "      if len(common_movies) > limit:\n",
        "        # calculate avg and deviation\n",
        "        ratings_j = {movie:usermovie2rating[(j,movie)] for movie in movies_j}\n",
        "        avg_j = np.mean(list(ratings_j.values()))\n",
        "        dev_j = {movie:(rating-avg_j) for movie,rating in ratings_j.items()}\n",
        "        dev_j_values = np.array(list(dev_j.values()))\n",
        "        sigma_j = np.sqrt(dev_j_values.dot(dev_j_values))\n",
        "\n",
        "        #calculate correlation coefficient\n",
        "        numerator = sum(dev_i[m]*dev_j[m] for m in common_movies)\n",
        "       \n",
        "        w_ij = numerator / (sigma_i * sigma_j)\n",
        "\n",
        "        # insert into sorted list and truncate\n",
        "        # negate weight, because list is sorted 'ascending'\n",
        "        # maximum value (1) is 'closest'\n",
        "        sl.add(((-w_ij), j))\n",
        "        if len(sl) > K:\n",
        "          del sl[-1]\n",
        "  neighbours.append(sl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5MQTDE99b-g"
      },
      "outputs": [],
      "source": [
        "# using neighbours, calculate train and test MSE\n",
        "def predict(i, m):\n",
        "  # calculate the weighted sum of deviations\n",
        "  numerator = 0\n",
        "  denominator = 0\n",
        "  for neg_w, j in neighbours[i]:\n",
        "    # remember, the weights are stored as its negative\n",
        "    # so the negative of the negative is the real positive weight\n",
        "    try:\n",
        "      numerator += -neg_w * deviations[j][m]\n",
        "      denominator += abs(neg_w)\n",
        "    except KeyError:\n",
        "      # neighbour may not have rated the same movie\n",
        "      # don't want to do dictionary look up twice\n",
        "      # so just throw exception\n",
        "      pass\n",
        "    if denominator == 0:\n",
        "      prediction = averages[i]\n",
        "    else:\n",
        "      prediction = numerator/denominator + averages[i]\n",
        "    \n",
        "    prediction = min(5, prediction)\n",
        "    prediction = max(0.5, prediction) # max rating is 0.5\n",
        "    \n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iwWzhLEVE-u"
      },
      "source": [
        "# making prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh_ANvO1ULF7"
      },
      "outputs": [],
      "source": [
        "train_predictions = []\n",
        "train_targets = []\n",
        "\n",
        "for (i,m), target in usermovie2rating.items():\n",
        "  #calculate the prediction for this movie\n",
        "  prediction = predict(i, m)\n",
        "\n",
        "  # save the prediction and target\n",
        "  train_predictions.append(prediction)\n",
        "  train_targets.append(target)\n",
        "\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "\n",
        "for (i,m), target in usermovie2rating_test.items():\n",
        "  prediction = predict(i, m)\n",
        "\n",
        "  test_predictions.append(prediction)\n",
        "  test_targets.append(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWRjkZ01VH3p"
      },
      "source": [
        "# evaluating predictions with mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19R4nU81VD-a"
      },
      "outputs": [],
      "source": [
        "def mse(p, t):\n",
        "  p = np.array(p)\n",
        "  t = np.array(t)\n",
        "  return np.mean((p-t)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrEcx3OIVfwK"
      },
      "outputs": [],
      "source": [
        "print('train mse:', mse(train_predictions, train_targets))\n",
        "print('test mse:', mse(test_predictions, test_targets))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "movielens_collaborative_filtering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
